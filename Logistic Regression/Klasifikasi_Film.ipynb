{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74814605-bf76-4c49-88df-217acf0bc92e",
   "metadata": {},
   "source": [
    "# Menginstal Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9716cd4-4c60-473a-be27-61ca84828570",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install nlpaug\n",
    "!pip install Sastrawi\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8da44-7495-4d0f-a8ea-4a05545bb2e6",
   "metadata": {},
   "source": [
    "# Load & Cek Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5095145-4992-44f6-9457-8a87443cd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66d9fc46-112b-4a39-af51-7e6bfa57b7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       film  \\\n",
      "0  Aku Tahu Kapan Kamu Mati   \n",
      "1                 Dignitate   \n",
      "2           Guru-Guru Gokil   \n",
      "3                     Janin   \n",
      "4                Mangkujiwo   \n",
      "\n",
      "                                            sinopsis   genre  \n",
      "0  Setelah kematian yang tampak, Siena mampu meli...   Horor  \n",
      "1  Alfi (Al Ghazali) bertemu dengan Alana (Caitli...   Drama  \n",
      "2  Ketika gaji staf di sekolahnya dicuri, seorang...  Komedi  \n",
      "3  Randu (Reuben Elishama Hadju) dan Dinar (Jill ...   Horor  \n",
      "4  Lahir dari Kuntilanak dari Cermin Kembar denga...   Horor  \n",
      "                          film  \\\n",
      "1733           Winter in Tokyo   \n",
      "1734           Petualang Cinta   \n",
      "1735          Last Night (III)   \n",
      "1736             Path of Light   \n",
      "1737  Kulihat cinta di matanya   \n",
      "\n",
      "                                               sinopsis     genre  \n",
      "1733  Winter in Tokyo berpusat pada kehidupan Ishida...  Romantis  \n",
      "1734  Markonah melarikan diri ke Jakarta karena akan...  Romantis  \n",
      "1735  Tempat pengambilan lebih dari 36 jam, Last Nig...  Romantis  \n",
      "1736  Proyek ini adalah tentang seorang lelaki Indo-...  Romantis  \n",
      "1737  Atika (Meriam Bellina) mantan penyanyi tenar, ...  Romantis  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1738 entries, 0 to 1737\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   film      1738 non-null   object\n",
      " 1   sinopsis  1738 non-null   object\n",
      " 2   genre     1738 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 40.9+ KB\n",
      "None\n",
      "film        0\n",
      "sinopsis    0\n",
      "genre       0\n",
      "dtype: int64\n",
      "Distribusi Genre (Jumlah Film per Genre):\n",
      "genre\n",
      "Drama       510\n",
      "Komedi      374\n",
      "Horor       349\n",
      "Laga        297\n",
      "Romantis    208\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datafile = pd.read_csv('movies.csv')\n",
    "print(datafile.head())\n",
    "print(datafile.tail())\n",
    "print(datafile.info()) # Menampilkan tipe data dan info memori\n",
    "print(datafile.isnull().sum()) # Mengecek jumlah data kosong di setiap kolom\n",
    "\n",
    "genre_distribution = datafile['genre'].value_counts() # Menghitung frekuensi tiap genre\n",
    "print(\"Distribusi Genre (Jumlah Film per Genre):\")\n",
    "print(genre_distribution)\n",
    "\n",
    "genre_distribution.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribusi Genre Film')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Jumlah Film')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d03f1-ccb6-4eb1-862e-17cb3720c8ed",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e9af5be-b308-414a-985d-1e0a49d73926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mahda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mahda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string, re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f73cdb-a245-4761-b405-b9fadc394183",
   "metadata": {},
   "source": [
    "Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c59571c3-6b52-4a2e-8b91-57f0b961c737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              lowercase\n",
      "0     setelah kematian yang tampak, siena mampu meli...\n",
      "1     alfi (al ghazali) bertemu dengan alana (caitli...\n",
      "2     ketika gaji staf di sekolahnya dicuri, seorang...\n",
      "3     randu (reuben elishama hadju) dan dinar (jill ...\n",
      "4     lahir dari kuntilanak dari cermin kembar denga...\n",
      "...                                                 ...\n",
      "1733  winter in tokyo berpusat pada kehidupan ishida...\n",
      "1734  markonah melarikan diri ke jakarta karena akan...\n",
      "1735  tempat pengambilan lebih dari 36 jam, last nig...\n",
      "1736  proyek ini adalah tentang seorang lelaki indo-...\n",
      "1737  atika (meriam bellina) mantan penyanyi tenar, ...\n",
      "\n",
      "[1738 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "datafile['lowercase'] = datafile['sinopsis'].str.lower()\n",
    "\n",
    "casefolding = pd.DataFrame(datafile['lowercase'])\n",
    "print(casefolding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17f58b3-83cb-46be-bd18-16f0b4c5e310",
   "metadata": {},
   "source": [
    "Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb73950-8f3a-41c7-b5e5-758270057f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             cleanpunct\n",
      "0     setelah kematian yang tampak siena mampu melih...\n",
      "1     alfi al ghazali bertemu dengan alana caitlin h...\n",
      "2     ketika gaji staf di sekolahnya dicuri seorang ...\n",
      "3     randu reuben elishama hadju dan dinar jill gla...\n",
      "4     lahir dari kuntilanak dari cermin kembar denga...\n",
      "...                                                 ...\n",
      "1733  winter in tokyo berpusat pada kehidupan ishida...\n",
      "1734  markonah melarikan diri ke jakarta karena akan...\n",
      "1735  tempat pengambilan lebih dari 36 jam last nigh...\n",
      "1736  proyek ini adalah tentang seorang lelaki indo ...\n",
      "1737  atika meriam bellina mantan penyanyi tenar ber...\n",
      "\n",
      "[1738 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuations(text):\n",
    "    clean_spcl = re.compile('[/(){}\\[\\]\\|@,;]') \n",
    "    clean_symbol = re.compile('[^0-9a-z]')\n",
    "    text = clean_spcl.sub('', text)\n",
    "    text = clean_symbol.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "datafile['cleanpunct'] = datafile['lowercase'].apply(remove_punctuations)\n",
    "cleanpunct = pd.DataFrame(datafile['cleanpunct'])\n",
    "print(cleanpunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1278c-f506-4752-adcc-53069fe86632",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7a4a9b0-f06f-4f94-a793-eb6ba5e004f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tokens\n",
      "0     [setelah, kematian, yang, tampak, siena, mampu...\n",
      "1     [alfi, al, ghazali, bertemu, dengan, alana, ca...\n",
      "2     [ketika, gaji, staf, di, sekolahnya, dicuri, s...\n",
      "3     [randu, reuben, elishama, hadju, dan, dinar, j...\n",
      "4     [lahir, dari, kuntilanak, dari, cermin, kembar...\n",
      "...                                                 ...\n",
      "1733  [winter, in, tokyo, berpusat, pada, kehidupan,...\n",
      "1734  [markonah, melarikan, diri, ke, jakarta, karen...\n",
      "1735  [tempat, pengambilan, lebih, dari, 36, jam, la...\n",
      "1736  [proyek, ini, adalah, tentang, seorang, lelaki...\n",
      "1737  [atika, meriam, bellina, mantan, penyanyi, ten...\n",
      "\n",
      "[1738 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def tokenisasi(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "datafile['tokens'] = datafile['cleanpunct'].apply(tokenisasi)\n",
    "tokenize = pd.DataFrame(datafile['tokens'])\n",
    "print(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4423e-90b0-4070-b4ed-ec2890218604",
   "metadata": {},
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad33beef-36e3-4f93-ae79-e7a9fc60392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               stopword\n",
      "0     [kematian, siena, tanda, tanda, orang, orang, ...\n",
      "1     [alfi, al, ghazali, bertemu, alana, caitlin, h...\n",
      "2     [gaji, staf, sekolahnya, dicuri, guru, enggan,...\n",
      "3     [randu, reuben, elishama, hadju, dinar, jill, ...\n",
      "4     [lahir, kuntilanak, cermin, kembar, kesengsara...\n",
      "...                                                 ...\n",
      "1733  [winter, tokyo, berpusat, kehidupan, ishida, k...\n",
      "1734  [markonah, melarikan, jakarta, dijodohkan, ora...\n",
      "1735  [pengambilan, 36, jam, last, night, pasangan, ...\n",
      "1736  [proyek, lelaki, indo, trinidadian, india, men...\n",
      "1737  [atika, meriam, bellina, mantan, penyanyi, ten...\n",
      "\n",
      "[1738 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def clean_stopwords(tokens):\n",
    "    stopword_id= set(stopwords.words('indonesian'))\n",
    "    stopword_eng = set(stopwords.words('english'))\n",
    "    text = [word for word in tokens if word not in stopword_id and word not in stopword_eng]\n",
    "    return text\n",
    "\n",
    "datafile['stopword'] = datafile['tokens'].apply(clean_stopwords)\n",
    "stopword = pd.DataFrame(datafile['stopword'])\n",
    "print(stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e253ff8-3e5b-4356-b731-5c1e63b5b193",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0d2b9dc-60bb-47d7-86d6-875bc5a03bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               stemming\n",
      "0     mati siena tanda tanda orang orang tinggal tem...\n",
      "1     alf al ghazali temu alana caitlin halderman si...\n",
      "2     gaji staf sekolah curi guru enggan usaha uang ...\n",
      "3     randu reuben elishama hadju dinar jill gladys ...\n",
      "4               lahir kuntilanak cermin kembar sengsara\n",
      "...                                                 ...\n",
      "1733  winter tokyo pusat hidup ishida keiko pamela b...\n",
      "1734  markonah lari jakarta jodoh orang tua lelaki t...\n",
      "1735  ambil 36 jam last night pasang meni joanna mic...\n",
      "1736  proyek lelaki indo trinidadian india cari akar...\n",
      "1737  atika meriam bellina mantan nyanyi tenar usaha...\n",
      "\n",
      "[1738 rows x 1 columns]\n",
      "File berhasil disimpan.\n"
     ]
    }
   ],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def stemming(tokens):\n",
    "   stems = [stemmer.stem(word) for word in tokens]\n",
    "   return ' '.join(stems)\n",
    "\n",
    "datafile['stemming'] = datafile['stopword'].apply(stemming)\n",
    "stemming = pd.DataFrame(datafile['stemming'])\n",
    "print(stemming)\n",
    "\n",
    "datafile[['genre', 'sinopsis', 'lowercase', 'cleanpunct', 'tokens', 'stopword', 'stemming']].to_csv('preprocessing2.csv', index=False)\n",
    "print(\"File berhasil disimpan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44fe2ce-4a53-4efd-bc80-d121e329ed6d",
   "metadata": {},
   "source": [
    "# Augmentasi Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d18ab6eb-b813-4adf-93df-8b84b234aa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File berhasil disimpan.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Fungsi Load Kamus\n",
    "def load_kamus(csv_path):\n",
    "    datafile = pd.read_csv(csv_path)\n",
    "    datafile.columns = datafile.columns.str.strip().str.lower()\n",
    "    kamus = {}\n",
    "    for _, row in datafile.iterrows():\n",
    "        base = row['kata'].strip().lower()\n",
    "        kamus[base] = []\n",
    "        for col in ['sinonim1', 'sinonim2', 'sinonim3']:\n",
    "            if pd.notna(row[col]):\n",
    "                kamus[base].append(row[col].strip().lower())\n",
    "    return kamus\n",
    "\n",
    "# Fungsi Augmentasi\n",
    "def aug_sinonim_kalimat(text, kamus, n=3):\n",
    "    words = text.lower().split()\n",
    "    new_words = words.copy()\n",
    "    \n",
    "    kandidat = [w for w in words if w in kamus and kamus[w]]\n",
    "    if not kandidat:\n",
    "        return text\n",
    "        \n",
    "    random.shuffle(kandidat)\n",
    "    kandidat = kandidat[:min(n, len(kandidat))]\n",
    "    \n",
    "    for word in kandidat:\n",
    "        if word in kamus and kamus[word]:\n",
    "            synonym = random.choice(kamus[word])\n",
    "            new_words = [synonym if w == word else w for w in new_words]\n",
    "    \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# Load Data Mentah dan Kamus\n",
    "datafile = pd.read_csv('movies.csv')\n",
    "kamus = load_kamus('kamus_sinonim.csv')\n",
    "\n",
    "# Target Jumlah Data per Genre\n",
    "target_count = 510\n",
    "augmented_only = []\n",
    "\n",
    "for genre in datafile['genre'].unique():\n",
    "    genre_df = datafile[datafile['genre'] == genre]\n",
    "    current_count = len(genre_df)\n",
    "    needed = target_count - current_count\n",
    "    \n",
    "    if needed <= 0:\n",
    "        continue\n",
    "\n",
    "    success_count = 0\n",
    "    attempts = 0\n",
    "    max_attempts = needed * 5\n",
    "\n",
    "    while success_count < needed and attempts < max_attempts:\n",
    "        row = genre_df.sample(n=1, replace=True).iloc[0]\n",
    "        sentence = row['sinopsis']\n",
    "        augmented_sentence = aug_sinonim_kalimat(sentence, kamus, n=3)\n",
    "\n",
    "        if augmented_sentence != sentence:\n",
    "            augmented_only.append(pd.DataFrame({\n",
    "                'film': [row['film'] + ' (Aug)'],\n",
    "                'sinopsis': [augmented_sentence],\n",
    "                'genre': [row['genre']]\n",
    "            }))\n",
    "            success_count += 1\n",
    "                \n",
    "# Simpan Hanya Hasil Augmentasi\n",
    "augmented_df = pd.concat(augmented_only, ignore_index=True)\n",
    "augmented_df.to_csv('sinopsis_augmented.csv', index=False)\n",
    "print(\"File berhasil disimpan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d88b3d-ee47-4a1b-83f6-283b577aa642",
   "metadata": {},
   "source": [
    "# Gabung Data Hasil Preprocessing & Data Hasil Augmentasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39dc40b4-4c8b-48e9-9ef5-c24bf34d9990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File berhasil disimpan.\n"
     ]
    }
   ],
   "source": [
    "datafile1 = pd.read_csv('preprocessing1.csv')\n",
    "datafile2 = pd.read_csv('preprocessing2.csv')\n",
    "\n",
    "final_datafile = pd.concat([datafile1, datafile2], ignore_index=True)\n",
    "\n",
    "final_datafile.to_csv('dataset_final_preprocessing.csv', index=False)\n",
    "print(\"File berhasil disimpan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309b9ebb-000f-486c-a683-f7042e1f38b7",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38aff3cc-d016-4bfc-b495-311fb49a89b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data training: 1784\n",
      "Jumlah data validation: 383\n",
      "Jumlah data testing: 383\n",
      "Total data: 2550\n",
      "\n",
      "Memeriksa duplikasi:\n",
      "Duplikat antara train dan validation: 31\n",
      "Duplikat antara train dan test: 38\n",
      "Duplikat antara validation dan test: 10\n",
      "Terlalu banyak duplikat\n",
      "\n",
      "Setelah perbaikan:\n",
      "Jumlah data training: 1679\n",
      "Jumlah data validation: 360\n",
      "Jumlah data testing: 360\n",
      "Total data: 2399\n",
      "\n",
      "Memeriksa duplikasi setelah perbaikan:\n",
      "Duplikat antara train dan validation: 0\n",
      "Duplikat antara train dan test: 0\n",
      "Duplikat antara validation dan test: 0\n",
      "File berhasil disimpan.\n",
      "\n",
      "Distribusi Genre:\n",
      "Train set:\n",
      "genre\n",
      "Drama       21.143538\n",
      "Horor       20.547945\n",
      "Komedi      20.369267\n",
      "Laga        19.475878\n",
      "Romantis    18.463371\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation set:\n",
      "genre\n",
      "Drama       21.111111\n",
      "Horor       20.555556\n",
      "Komedi      20.277778\n",
      "Laga        19.444444\n",
      "Romantis    18.611111\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set:\n",
      "genre\n",
      "Drama       21.388889\n",
      "Horor       20.555556\n",
      "Komedi      20.277778\n",
      "Laga        19.444444\n",
      "Romantis    18.333333\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Baca Dataset\n",
    "datafile = pd.read_csv('dataset_final_preprocessing.csv') \n",
    "\n",
    "# Rasio 70:15:15\n",
    "TRAIN_RATIO = 0.7\n",
    "TEST_RATIO = 0.5\n",
    "\n",
    "X = datafile['stemming']  \n",
    "y = datafile['genre'] \n",
    "\n",
    "# Split Data \n",
    "# Split Data Train & (Validation + Test)\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(\n",
    "    X, y, test_size=(1-TRAIN_RATIO), random_state=42, stratify=y)\n",
    "\n",
    "# Split Data Validation & Test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_valtest, y_valtest, test_size=TEST_RATIO, random_state=42, stratify=y_valtest)\n",
    "\n",
    "# Simpan Data\n",
    "train_df = pd.DataFrame({'stemming': X_train, 'genre': y_train})\n",
    "val_df = pd.DataFrame({'stemming': X_val, 'genre': y_val})\n",
    "test_df = pd.DataFrame({'stemming': X_test, 'genre': y_test})\n",
    "\n",
    "# Periksa Jumlah & Duplikasi Data\n",
    "print(f\"Jumlah data training: {len(train_df)}\")\n",
    "print(f\"Jumlah data validation: {len(val_df)}\")\n",
    "print(f\"Jumlah data testing: {len(test_df)}\")\n",
    "print(f\"Total data: {len(train_df) + len(val_df) + len(test_df)}\")\n",
    "\n",
    "train_val_duplicates = val_df['stemming'].isin(train_df['stemming']).sum()\n",
    "train_test_duplicates = test_df['stemming'].isin(train_df['stemming']).sum()\n",
    "val_test_duplicates = test_df['stemming'].isin(val_df['stemming']).sum()\n",
    "\n",
    "print(\"Memeriksa duplikasi:\")\n",
    "print(f\"Duplikat antara train dan validation: {train_val_duplicates}\")\n",
    "print(f\"Duplikat antara train dan test: {train_test_duplicates}\")\n",
    "print(f\"Duplikat antara validation dan test: {val_test_duplicates}\")\n",
    "\n",
    "if max(train_val_duplicates/len(val_df), train_test_duplicates/len(test_df), \n",
    "       val_test_duplicates/len(test_df)) > 0.05:  # duplikat > 5%\n",
    "    print(\"Terlalu banyak duplikat\")\n",
    "    \n",
    "    # Hapus Duplikasi dari Dataset Asli\n",
    "    datafile_unique = datafile.drop_duplicates(subset=['stemming'])\n",
    "    X_unique = datafile_unique['stemming']\n",
    "    y_unique = datafile_unique['genre']\n",
    "    \n",
    "    # Split Data\n",
    "    X_train, X_valtest, y_train, y_valtest = train_test_split(\n",
    "        X_unique, y_unique, test_size=(1-TRAIN_RATIO), random_state=42, stratify=y_unique)\n",
    "    \n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_valtest, y_valtest, test_size=TEST_RATIO, random_state=42, stratify=y_valtest)\n",
    "    \n",
    "    # Simpan Data\n",
    "    train_df = pd.DataFrame({'stemming': X_train, 'genre': y_train})\n",
    "    val_df = pd.DataFrame({'stemming': X_val, 'genre': y_val})\n",
    "    test_df = pd.DataFrame({'stemming': X_test, 'genre': y_test})\n",
    "    \n",
    "    # Periksa Data Setelah Perbaikan\n",
    "    print(\"Setelah perbaikan:\")\n",
    "    print(f\"Jumlah data training: {len(train_df)}\")\n",
    "    print(f\"Jumlah data validation: {len(val_df)}\")\n",
    "    print(f\"Jumlah data testing: {len(test_df)}\")\n",
    "    print(f\"Total data: {len(train_df) + len(val_df) + len(test_df)}\")\n",
    "    \n",
    "    # Periksa Duplikasi\n",
    "    train_val_duplicates = val_df['stemming'].isin(train_df['stemming']).sum()\n",
    "    train_test_duplicates = test_df['stemming'].isin(train_df['stemming']).sum()\n",
    "    val_test_duplicates = test_df['stemming'].isin(val_df['stemming']).sum()\n",
    "    \n",
    "    print(\"Memeriksa duplikasi setelah perbaikan:\")\n",
    "    print(f\"Duplikat antara train dan validation: {train_val_duplicates}\")\n",
    "    print(f\"Duplikat antara train dan test: {train_test_duplicates}\")\n",
    "    print(f\"Duplikat antara validation dan test: {val_test_duplicates}\")\n",
    "\n",
    "# Simpan Hasil Split Data\n",
    "train_df.to_csv('data_train.csv', index=False)\n",
    "val_df.to_csv('data_validation.csv', index=False)\n",
    "test_df.to_csv('data_test.csv', index=False)\n",
    "print(\"File berhasil disimpan.\")\n",
    "\n",
    "# Distribusi Genre Setiap Set Data\n",
    "print(\"Distribusi Genre:\")\n",
    "print(\"Train set:\")\n",
    "print(train_df['genre'].value_counts(normalize=True).sort_index() * 100)\n",
    "print(\"Validation set:\")\n",
    "print(val_df['genre'].value_counts(normalize=True).sort_index() * 100)\n",
    "print(\"Test set:\")\n",
    "print(test_df['genre'].value_counts(normalize=True).sort_index() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fbb67b-6770-4695-bae0-430fe5a29d79",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3d66da4-e500-47d2-898a-af8ac7d445c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File berhasil disimpan.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Baca Dataset\n",
    "datafile = pd.read_csv('data_validation.csv')\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "datafile['label'] = datafile.genre.map({\n",
    "    'Horor' : 0,\n",
    "    'Drama' : 1,\n",
    "    'Komedi' : 2,\n",
    "    'Laga' : 3,\n",
    "    'Romantis' : 4\n",
    "})\n",
    "\n",
    "# Simpan Hasil Label\n",
    "datafile[['label', 'stemming']].to_csv('hasil_labeling_validation.csv', index=False)\n",
    "print(\"File berhasil disimpan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdba738-df8d-4f23-a78b-dcb5227c6bbf",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9797fdcb-8f0f-48fe-a077-2c746f1f849e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Kata dengan skor TF-IDF tertinggi:\n",
      "            kata  skor_tfidf\n",
      "4287       cinta    0.017600\n",
      "7679       hidup    0.016979\n",
      "6129        film    0.016397\n",
      "1088        anak    0.015859\n",
      "21650       temu    0.015382\n",
      "15243      orang    0.014963\n",
      "18312      rumah    0.014733\n",
      "22106    tinggal    0.013421\n",
      "21537      teman    0.012953\n",
      "6317       gadis    0.012761\n",
      "13748      milik    0.012700\n",
      "8934       jalan    0.011996\n",
      "11064      kisah    0.011905\n",
      "4091      cerita    0.011517\n",
      "18558    sahabat    0.011200\n",
      "8346   indonesia    0.011120\n",
      "1928        ayah    0.010548\n",
      "23500     wanita    0.010159\n",
      "3602       bunuh    0.010138\n",
      "14470       nama    0.010083\n",
      "\n",
      "Jumlah fitur (kata unik): 23889\n",
      "Data training TF-IDF shape: (1679, 23889)\n",
      "Data validation TF-IDF shape: (360, 23889)\n",
      "Data testing TF-IDF shape : (360, 23889)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Baca Dataset\n",
    "datafile1 = pd.read_csv('hasil_labeling_train.csv')\n",
    "datafile2 = pd.read_csv('hasil_labeling_validation.csv')\n",
    "datafile3 = pd.read_csv('hasil_labeling_test.csv')\n",
    "\n",
    "X_train_text = datafile1['stemming']\n",
    "y_train = datafile1['label']\n",
    "\n",
    "X_val_text = datafile2['stemming']\n",
    "y_val = datafile2['label']\n",
    "\n",
    "X_test_text = datafile3['stemming']\n",
    "y_test = datafile3['label']\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    ngram_range = (1,2)\n",
    ")\n",
    "\n",
    "X_train = tfidf.fit_transform(X_train_text )\n",
    "X_val = tfidf.transform(X_val_text)\n",
    "X_test = tfidf.transform(X_test_text)\n",
    "\n",
    "# Proses Keywords\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "def extract_top_keywords(doc_vector, feature_names, top_n=10):\n",
    "    indices = np.argsort(doc_vector)[::-1][:top_n]\n",
    "    return [feature_names[i] for i in indices]\n",
    "\n",
    "all_keywords = []\n",
    "dense_train = X_train.toarray()\n",
    "for vector in dense_train:\n",
    "    keywords = extract_top_keywords(vector, feature_names)\n",
    "    all_keywords.append(keywords)\n",
    "\n",
    "# Simpan Keywords\n",
    "datafile_keywords = pd.DataFrame({\n",
    "    'stemming': X_train_text,\n",
    "    'keywords': all_keywords,\n",
    "})\n",
    "datafile_keywords.to_csv('hasil_keywords_train.csv', index=False)\n",
    "\n",
    "# Simpan Hasil TF-IDF Training, Validation, & Testing\n",
    "tfidf_train_df = pd.DataFrame(dense_train, columns=feature_names)\n",
    "tfidf_train_df['label'] = y_train.values\n",
    "tfidf_train_df.to_csv('hasil_tfidf_train.csv', index=False)\n",
    "\n",
    "dense_val = X_val.toarray()\n",
    "tfidf_validation_df = pd.DataFrame(dense_val, columns=feature_names)\n",
    "tfidf_validation_df['label'] = y_val.values\n",
    "tfidf_validation_df.to_csv('hasil_tfidf_validation.csv', index=False)\n",
    "\n",
    "dense_test = X_test.toarray()\n",
    "tfidf_test_df = pd.DataFrame(dense_test, columns=feature_names)\n",
    "tfidf_test_df['label'] = y_test.values\n",
    "tfidf_test_df.to_csv('hasil_tfidf_test.csv', index=False)\n",
    "\n",
    "mean_tfidf = X_train.mean(axis=0).A1\n",
    "\n",
    "tfidf_scores = pd.DataFrame({\n",
    "    'kata': feature_names,\n",
    "    'skor_tfidf': mean_tfidf\n",
    "})\n",
    "tfidf_scores = tfidf_scores.sort_values('skor_tfidf', ascending=False)\n",
    "\n",
    "# Menampilkan 20 Kata dengan Skor TF-IDF Tertinggi\n",
    "print(\"20 Kata dengan skor TF-IDF tertinggi:\")\n",
    "print(tfidf_scores.head(20))\n",
    "\n",
    "# Menampilkan Jumlah Fitur\n",
    "print(\"\\nJumlah fitur (kata unik):\", len(feature_names))\n",
    "print(\"Data training TF-IDF shape:\", X_train.shape)\n",
    "print(\"Data validation TF-IDF shape:\", X_val.shape)\n",
    "print(\"Data testing TF-IDF shape :\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d718f-7a00-4b75-9b59-bae2a2957a77",
   "metadata": {},
   "source": [
    "# Reduksi Dimensi (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6209c0df-37fe-440f-83e7-4f28fac97785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA dengan 167 fitur\n",
      "Total varians yang dijelaskan dengan 167 fitur: 0.7758 atau 77.58%\n",
      "Sebelum PCA (train): (1679, 23889)  Setelah PCA: (1679, 167)\n",
      "Sebelum PCA (validation): (360, 23889)  Setelah PCA: (360, 167)\n",
      "Sebelum PCA (test): (360, 23889)  Setelah PCA: (360, 167)\n",
      "PCA dengan 300 fitur\n",
      "Total varians yang dijelaskan dengan 300 fitur: 0.8285 atau 82.85%\n",
      "Sebelum PCA (train): (1679, 23889)  Setelah PCA: (1679, 300)\n",
      "Sebelum PCA (validation): (360, 23889)  Setelah PCA: (360, 300)\n",
      "Sebelum PCA (test): (360, 23889)  Setelah PCA: (360, 300)\n",
      "PCA dengan 500 fitur\n",
      "Total varians yang dijelaskan dengan 500 fitur: 0.8900 atau 89.00%\n",
      "Sebelum PCA (train): (1679, 23889)  Setelah PCA: (1679, 500)\n",
      "Sebelum PCA (validation): (360, 23889)  Setelah PCA: (360, 500)\n",
      "Sebelum PCA (test): (360, 23889)  Setelah PCA: (360, 500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Baca Dataset\n",
    "X_train = pd.read_csv('hasil_tfidf_train.csv')\n",
    "X_val = pd.read_csv('hasil_tfidf_validation.csv')\n",
    "X_test = pd.read_csv('hasil_tfidf_test.csv')\n",
    "\n",
    "y_train = pd.read_csv('hasil_labeling_train.csv')['label']\n",
    "y_val = pd.read_csv('hasil_labeling_validation.csv')['label']\n",
    "y_test = pd.read_csv('hasil_labeling_test.csv')['label']\n",
    "\n",
    "# PCA\n",
    "fitur_list = [167, 300, 500]\n",
    "\n",
    "for n in fitur_list:\n",
    "    print(f\"PCA dengan {n} fitur\")\n",
    "    \n",
    "    pca = PCA(n_components=n)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_val_pca = pca.transform(X_val)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    explained_var = np.sum(pca.explained_variance_ratio_)\n",
    "    print(f\"Total varians yang dijelaskan dengan {n} fitur: {explained_var:.4f} atau {explained_var*100:.2f}%\")\n",
    "\n",
    "# Simpan Hasil PCA\n",
    "    train_pca_df = pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(n)])\n",
    "    val_pca_df = pd.DataFrame(X_val_pca, columns=[f'PC{i+1}' for i in range(n)])\n",
    "    test_pca_df = pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(n)])\n",
    "\n",
    "    train_pca_df['label'] = y_train.values\n",
    "    val_pca_df['label'] = y_val.values\n",
    "    test_pca_df['label'] = y_test.values\n",
    "\n",
    "    train_pca_df.to_csv(f'hasil_pca_train_{n}.csv', index=False)\n",
    "    val_pca_df.to_csv(f'hasil_pca_validation_{n}.csv', index=False)\n",
    "    test_pca_df.to_csv(f'hasil_pca_test_{n}.csv', index=False)\n",
    "\n",
    "#Tampilkan Hasil Varians\n",
    "    print(f\"Sebelum PCA (train): {X_train.shape}  Setelah PCA: {X_train_pca.shape}\")\n",
    "    print(f\"Sebelum PCA (validation): {X_val.shape}  Setelah PCA: {X_val_pca.shape}\")\n",
    "    print(f\"Sebelum PCA (test): {X_test.shape}  Setelah PCA: {X_test_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8eb7a-083d-4af5-a818-20cc84f46b84",
   "metadata": {},
   "source": [
    "# Model Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c59a7d9c-a436-4fd3-8afa-298ab3d97e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def one_hot(y, num_classes):\n",
    "    return np.eye(num_classes)[y]\n",
    "    \n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "    \n",
    "class MulticlassLogisticRegression():\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000, batch_size=32, lambda_reg=0.01):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.batch_size = batch_size\n",
    "        self.lambda_reg = lambda_reg  # L2 regularization\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.best_weights = None\n",
    "        self.best_bias = None\n",
    "        self.classes_ = None\n",
    "        self.scaler = None\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None, verbose=1):   \n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        # Inisialisasi Weights\n",
    "        self.weights = np.random.randn(n_features, n_classes) * np.sqrt(2.0 / (n_features + n_classes))\n",
    "        self.bias = np.zeros(n_classes)\n",
    "        \n",
    "        y_one_hot = one_hot(y, n_classes)\n",
    "\n",
    "        self.best_val_accuracy = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for i in range(self.n_iters):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            batch_losses = []\n",
    "            \n",
    "            for start_idx in range(0, n_samples, self.batch_size):\n",
    "                batch_indices = indices[start_idx:min(start_idx + self.batch_size, n_samples)]\n",
    "                X_batch = X[batch_indices]\n",
    "                y_batch_one_hot = y_one_hot[batch_indices]\n",
    "                \n",
    "                # Forward Pass\n",
    "                linear_output = np.dot(X_batch, self.weights) + self.bias\n",
    "                probs = softmax(linear_output)\n",
    "                \n",
    "                # Compute loss (Cross-Entropy)\n",
    "                batch_loss = -np.mean(np.sum(y_batch_one_hot * np.log(probs + 1e-10), axis=1))\n",
    "                batch_loss += 0.5 * self.lambda_reg * np.sum(self.weights**2)  # L2 penalty\n",
    "                batch_losses.append(batch_loss)\n",
    "                \n",
    "                # Gradient dengan Regularisasi L2\n",
    "                dw = np.dot(X_batch.T, (probs - y_batch_one_hot)) / len(batch_indices)\n",
    "                dw += self.lambda_reg * self.weights  # L2 regularization\n",
    "                db = np.sum(probs - y_batch_one_hot, axis=0) / len(batch_indices)\n",
    "                \n",
    "                # Update weights dengan learning rate\n",
    "                self.weights = self.weights - self.lr * dw\n",
    "                self.bias = self.bias - self.lr * db\n",
    "            \n",
    "            # Train Accuracy\n",
    "            train_preds = self.predict(X)\n",
    "            train_acc = accuracy(y, train_preds)\n",
    "            train_loss = np.mean(batch_losses)\n",
    "            \n",
    "            # Validasi Matrix Calculation\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_probs = self.predict_proba(X_val)\n",
    "                val_preds = np.argmax(val_probs, axis=1)\n",
    "                val_accuracy = accuracy(y_val, val_preds)\n",
    "                \n",
    "                # Menghitung Validation Loss\n",
    "                y_val_one_hot = one_hot(y_val, n_classes)\n",
    "                val_loss = -np.mean(np.sum(y_val_one_hot * np.log(val_probs + 1e-10), axis=1))\n",
    "                val_loss += 0.5 * self.lambda_reg * np.sum(self.weights**2)\n",
    "                \n",
    "                if val_accuracy > self.best_val_accuracy:\n",
    "                    self.best_val_accuracy = val_accuracy\n",
    "                    self.best_weights = self.weights.copy()\n",
    "                    self.best_bias = self.bias.copy()\n",
    "            \n",
    "        # Menggunakan Best Weights\n",
    "        if X_val is None and y_val is None:\n",
    "            self.best_weights = self.weights.copy()\n",
    "            self.best_bias = self.bias.copy()\n",
    "            \n",
    "        self.weights = self.best_weights\n",
    "        self.bias = self.best_bias\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Prediksi Kelas probabilitas\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return softmax(linear_output)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Prediksi Kelas Label\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def evaluate(self, X, y, verbose=1):\n",
    "        # Evaluasi Performa Model\n",
    "        y_pred = self.predict(X)\n",
    "        acc = accuracy(y, y_pred)\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(f\"Accuracy: {acc:.4f}\")\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y, y_pred))\n",
    "            print(\"\\nConfusion Matrix:\")\n",
    "            print(confusion_matrix(y, y_pred))\n",
    "            \n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8d388-341b-42b4-88b9-9c3679f83fff",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "274a488c-c212-4408-8677-53f0ab6fc6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model_class, param_grid, X, y, n_splits=5, random_state=42):\n",
    "    results = []\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    \n",
    "    # Membuat Kombinasi Parameter\n",
    "    param_combinations = []\n",
    "    keys = list(param_grid.keys())\n",
    "    values = list(param_grid.values())\n",
    "    \n",
    "    def generate_combinations(combination, index):\n",
    "        if index == len(keys):\n",
    "            param_combinations.append(combination.copy())\n",
    "            return\n",
    "        \n",
    "        for value in values[index]:\n",
    "            combination[keys[index]] = value\n",
    "            generate_combinations(combination, index + 1)\n",
    "    \n",
    "    generate_combinations({}, 0)\n",
    "\n",
    "    # Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Evaluasi Kombinasi Parameter\n",
    "    for i, params in enumerate(param_combinations):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nEvaluating parameters {i+1}/{len(param_combinations)}: {params}\")\n",
    "\n",
    "        # Cross-validation untuk setiap kombinasi parameter\n",
    "        fold_scores = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "            X_fold_train, X_fold_val = X[train_idx], X[val_idx]\n",
    "            y_fold_train, y_fold_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "        # Buat dan Train Model\n",
    "        model = model_class(**params)\n",
    "        model.fit(X_fold_train, y_fold_train, X_fold_val, y_fold_val, verbose=0)\n",
    "        \n",
    "        # Evaluasi dengan Data Validation\n",
    "        val_score = model.evaluate(X_fold_val, y_fold_val, verbose=0)\n",
    "        fold_scores.append(val_score)\n",
    "        \n",
    "        # Rata-rata skor dari semua fold\n",
    "        mean_val_score = np.mean(fold_scores)\n",
    "        std_val_score = np.std(fold_scores)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        result = {\n",
    "            'params': params.copy(),\n",
    "            'mean_score': mean_val_score,\n",
    "            'std_score': std_val_score,\n",
    "            'fold_scores': fold_scores,\n",
    "            'time': elapsed_time\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"Mean Validation Accuracy: {mean_val_score:.4f}  {std_val_score:.4f} (Time: {elapsed_time:.2f}s)\")\n",
    "        \n",
    "        # Update Best Parameters\n",
    "        if mean_val_score > best_score:\n",
    "            best_score = mean_val_score\n",
    "            best_params = params.copy()\n",
    "    \n",
    "    print(\"\\nHasil Grid Search\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best validasi accuracy: {best_score:.4f}\")\n",
    "    \n",
    "    return best_params, best_score, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd4a6e8-61be-43ae-86d5-aa3179cf0a90",
   "metadata": {},
   "source": [
    "# Evaluasi Performa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3e4923cc-eef4-436b-98bd-f0902a806c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Fungsi Menghitung Akurasi Model\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)\n",
    "    return correct / len(y_true)\n",
    "\n",
    "# Fungsi Menghitung Confusion Matrix\n",
    "def calculate_confusion_matrix(y_true, y_pred, classes):\n",
    "    n_classes = len(classes)\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "    \n",
    "    # Inisialisasi confusion matrix\n",
    "    conf_matrix = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    \n",
    "    # Isi confusion matrix\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        true_idx = class_to_idx[true]\n",
    "        pred_idx = class_to_idx[pred]\n",
    "        conf_matrix[true_idx, pred_idx] += 1\n",
    "    \n",
    "    return conf_matrix\n",
    "\n",
    "# Fungsi Menghitung Precision, Recall, dan F1-score per Kelas\n",
    "def calculate_precision_recall_f1(conf_matrix, classes):\n",
    "    n_classes = len(classes)\n",
    "    precision = np.zeros(n_classes)\n",
    "    recall = np.zeros(n_classes)\n",
    "    f1_score = np.zeros(n_classes)\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        # True Positives adalah elemen diagonal\n",
    "        tp = conf_matrix[i, i]\n",
    "        \n",
    "        # False Positives adalah jumlah non-diagonal dalam kolom i\n",
    "        fp = np.sum(conf_matrix[:, i]) - tp\n",
    "        \n",
    "        # False Negatives adalah jumlah non-diagonal dalam baris i\n",
    "        fn = np.sum(conf_matrix[i, :]) - tp\n",
    "        \n",
    "        # Menghitung precision, recall, dan F1-score\n",
    "        precision[i] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall[i] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i]) if (precision[i] + recall[i]) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "# Fungsi Menghitung Macro F1-score\n",
    "def calculate_macro_f1(f1_scores):\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "# Visualisasi Confusion Matrix\n",
    "def plot_confusion_matrix(conf_matrix, classes, title='Confusion Matrix', cmap=None):\n",
    "    if cmap is None:\n",
    "        cmap = plt.cm.Blues\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Tambahkan nilai di setiap sel\n",
    "    thresh = conf_matrix.max() / 2.0\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            plt.text(j, i, format(conf_matrix[i, j], 'd'),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Simpan gambar\n",
    "    plt.savefig('confusion_matrix500.png')\n",
    "    plt.close()\n",
    "\n",
    "# Evaluasi Model dan Simpan Hasil ke File\n",
    "def evaluate_model(y_true, y_pred, classes, output_filename='model_evaluation_results.txt'):\n",
    "    # Hitung Akurasi\n",
    "    accuracy = calculate_accuracy(y_true, y_pred)\n",
    "    \n",
    "    # Hitung Confusion Matrix\n",
    "    conf_matrix = calculate_confusion_matrix(y_true, y_pred, classes)\n",
    "    \n",
    "    # Hitung Precision, Recall, dan F1-score\n",
    "    precision, recall, f1_score = calculate_precision_recall_f1(conf_matrix, classes)\n",
    "    \n",
    "    # Hitung Macro F1-score\n",
    "    macro_f1 = calculate_macro_f1(f1_score)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(conf_matrix, classes)\n",
    "    \n",
    "    # Tulis hasil ke file\n",
    "    with open(output_filename, 'w') as f:\n",
    "        f.write(\"Model Evaluation Results\\n\")\n",
    "        f.write(\"======================\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Overall Accuracy: {accuracy:.4f}\\n\")\n",
    "        f.write(f\"Macro F1-Score: {macro_f1:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(\"--------------\\n\")\n",
    "        f.write(\"\".join([f\"{cls:>12}\" for cls in classes]) + \"\\n\")\n",
    "        for i, row in enumerate(conf_matrix):\n",
    "            f.write(f\"{classes[i]:>10} \" + \"\".join([f\"{val:>12}\" for val in row]) + \"\\n\")\n",
    "        \n",
    "        f.write(\"\\nEvaluasi per Genre:\\n\")\n",
    "        f.write(\"------------------\\n\")\n",
    "        f.write(\"Genre      Precision    Recall    F1-Score\\n\")\n",
    "        for i, cls in enumerate(classes):\n",
    "            f.write(f\"{cls:10} {precision[i]:10.4f} {recall[i]:10.4f} {f1_score[i]:10.4f}\\n\")\n",
    "    \n",
    "    # Tampilkan Hasil di Konsol\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro F1-Score: {macro_f1:.4f}\")\n",
    "    print(\"\\nEvaluasi per Genre:\")\n",
    "    print(\"Genre      Precision    Recall    F1-Score\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        print(f\"{cls:10} {precision[i]:10.4f} {recall[i]:10.4f} {f1_score[i]:10.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'classes': classes\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480b22a-2d20-4c44-9424-1491f8905c10",
   "metadata": {},
   "source": [
    "# Visualisasi Hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "942ac052-c882-40c1-844e-2838c1509ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1679, 500) - Features: 500, Samples: 1679\n",
      "Validation data shape: (360, 500) - Features: 500, Samples: 360\n",
      "Test data shape: (360, 500) - Features: 500, Samples: 360\n",
      "Number of classes: 5\n",
      "\n",
      "Running Grid Search dengan Cross-Validation\n",
      "\n",
      "Evaluating parameters 1/18: {'learning_rate': 0.01, 'batch_size': 32, 'lambda_reg': 0.1, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.7045  0.0000 (Time: 13.82s)\n",
      "\n",
      "Evaluating parameters 2/18: {'learning_rate': 0.01, 'batch_size': 32, 'lambda_reg': 0.5, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.6985  0.0000 (Time: 13.85s)\n",
      "\n",
      "Evaluating parameters 3/18: {'learning_rate': 0.01, 'batch_size': 32, 'lambda_reg': 1.0, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.7104  0.0000 (Time: 12.90s)\n",
      "\n",
      "Evaluating parameters 4/18: {'learning_rate': 0.01, 'batch_size': 64, 'lambda_reg': 0.1, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.6955  0.0000 (Time: 12.09s)\n",
      "\n",
      "Evaluating parameters 5/18: {'learning_rate': 0.01, 'batch_size': 64, 'lambda_reg': 0.5, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.6955  0.0000 (Time: 15.07s)\n",
      "\n",
      "Evaluating parameters 6/18: {'learning_rate': 0.01, 'batch_size': 64, 'lambda_reg': 1.0, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.7015  0.0000 (Time: 14.77s)\n",
      "\n",
      "Evaluating parameters 7/18: {'learning_rate': 0.005, 'batch_size': 32, 'lambda_reg': 0.1, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.7104  0.0000 (Time: 13.36s)\n",
      "\n",
      "Evaluating parameters 8/18: {'learning_rate': 0.005, 'batch_size': 32, 'lambda_reg': 0.5, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.6896  0.0000 (Time: 16.00s)\n",
      "\n",
      "Evaluating parameters 9/18: {'learning_rate': 0.005, 'batch_size': 32, 'lambda_reg': 1.0, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.6925  0.0000 (Time: 16.72s)\n",
      "\n",
      "Evaluating parameters 10/18: {'learning_rate': 0.005, 'batch_size': 64, 'lambda_reg': 0.1, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.7045  0.0000 (Time: 12.88s)\n",
      "\n",
      "Evaluating parameters 11/18: {'learning_rate': 0.005, 'batch_size': 64, 'lambda_reg': 0.5, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.6925  0.0000 (Time: 13.24s)\n",
      "\n",
      "Evaluating parameters 12/18: {'learning_rate': 0.005, 'batch_size': 64, 'lambda_reg': 1.0, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.6955  0.0000 (Time: 13.31s)\n",
      "\n",
      "Evaluating parameters 13/18: {'learning_rate': 0.001, 'batch_size': 32, 'lambda_reg': 0.1, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.6925  0.0000 (Time: 16.46s)\n",
      "\n",
      "Evaluating parameters 14/18: {'learning_rate': 0.001, 'batch_size': 32, 'lambda_reg': 0.5, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.7045  0.0000 (Time: 15.86s)\n",
      "\n",
      "Evaluating parameters 15/18: {'learning_rate': 0.001, 'batch_size': 32, 'lambda_reg': 1.0, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.6955  0.0000 (Time: 16.02s)\n",
      "\n",
      "Evaluating parameters 16/18: {'learning_rate': 0.001, 'batch_size': 64, 'lambda_reg': 0.1, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.7194  0.0000 (Time: 11.81s)\n",
      "\n",
      "Evaluating parameters 17/18: {'learning_rate': 0.001, 'batch_size': 64, 'lambda_reg': 0.5, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.6955  0.0000 (Time: 13.91s)\n",
      "\n",
      "Evaluating parameters 18/18: {'learning_rate': 0.001, 'batch_size': 64, 'lambda_reg': 1.0, 'n_iters': 1000}\n",
      "Mean Validation Accuracy: 0.6866  0.0000 (Time: 12.77s)\n",
      "\n",
      "Hasil Grid Search\n",
      "Best parameters: {'learning_rate': 0.001, 'batch_size': 64, 'lambda_reg': 0.1, 'n_iters': 1000}\n",
      "Best validasi accuracy: 0.7194\n",
      "\n",
      "Training Final Model\n",
      "Best parameters: {'learning_rate': 0.001, 'batch_size': 64, 'lambda_reg': 0.1, 'n_iters': 1000}\n",
      "\n",
      "Evaluasi Final Model dengan Data Test\n",
      "Accuracy: 0.7667\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81        74\n",
      "           1       0.61      0.58      0.60        77\n",
      "           2       0.72      0.68      0.70        73\n",
      "           3       0.90      0.77      0.83        70\n",
      "           4       0.85      0.97      0.91        66\n",
      "\n",
      "    accuracy                           0.77       360\n",
      "   macro avg       0.77      0.77      0.77       360\n",
      "weighted avg       0.77      0.77      0.76       360\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[63 11  0  0  0]\n",
      " [16 45 14  2  0]\n",
      " [ 3 15 50  4  1]\n",
      " [ 0  2  4 54 10]\n",
      " [ 0  1  1  0 64]]\n",
      "\n",
      "Final Test Accuracy: 0.7667\n",
      "Overall Accuracy: 0.7667\n",
      "Macro F1-Score: 0.7693\n",
      "\n",
      "Evaluasi per Genre:\n",
      "Genre      Precision    Recall    F1-Score\n",
      "         0     0.7683     0.8514     0.8077\n",
      "         1     0.6081     0.5844     0.5960\n",
      "         2     0.7246     0.6849     0.7042\n",
      "         3     0.9000     0.7714     0.8308\n",
      "         4     0.8533     0.9697     0.9078\n",
      "Evaluasi model selesai. Hasil tersimpan di 'model_evaluation_results500.txt'\n",
      "Plot confusion matrix tersimpan di 'confusion_matrix500.png'\n",
      "\n",
      "Melakukan 5-fold Cross Validation untuk analisis stabilitas model\n",
      "Fold 1/5 Accuracy: 0.7745\n",
      "Fold 2/5 Accuracy: 0.7868\n",
      "Fold 3/5 Accuracy: 0.7525\n",
      "Fold 4/5 Accuracy: 0.7647\n",
      "Fold 5/5 Accuracy: 0.7862\n",
      "Akurasi rata-rata: 0.7729  0.0131\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Training Data Untuk Train Model\n",
    "train_data = pd.read_csv('hasil_pca_train_167.csv')\n",
    "target_column = 'label'\n",
    "X_train = train_data.drop(columns=[target_column]).values\n",
    "y_train = train_data[target_column].values\n",
    "\n",
    "# Load Validation Data Untuk Tuning Hyperparameter\n",
    "validation_data = pd.read_csv('hasil_pca_validation_167.csv')\n",
    "X_val = validation_data.drop(columns=[target_column]).values\n",
    "y_val = validation_data[target_column].values\n",
    "\n",
    "# Load Test Data Untuk Evaluasi Akhir\n",
    "test_data = pd.read_csv('hasil_pca_test_167.csv')\n",
    "X_test = test_data.drop(columns=[target_column]).values\n",
    "y_test = test_data[target_column].values\n",
    "\n",
    "# Standardisasi Fitur\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape} - Features: {X_train.shape[1]}, Samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation data shape: {X_val.shape} - Features: {X_val.shape[1]}, Samples: {X_val.shape[0]}\")\n",
    "print(f\"Test data shape: {X_test.shape} - Features: {X_test.shape[1]}, Samples: {X_test.shape[0]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
    "\n",
    "# Grid Search untuk Menemukan Hyperparameter Terbaik\n",
    "print(\"\\nRunning Grid Search dengan Cross-Validation\")\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.005, 0.001],\n",
    "    'batch_size': [32, 64],\n",
    "    'lambda_reg': [0.1, 0.5, 1.0],\n",
    "    'n_iters': [1000],\n",
    "}\n",
    "\n",
    "# Gunakan subset data untuk grid search jika datanya besar\n",
    "if X_train.shape[0] > 5000:\n",
    "    # Menggunakan 30% data untuk grid search\n",
    "    grid_search_size = int(X_train.shape[0] * 0.3)\n",
    "    indices = np.random.choice(X_train.shape[0], grid_search_size, replace=False)\n",
    "    X_grid, y_grid = X_train_scaled[indices], y_train[indices]\n",
    "else:\n",
    "    X_grid, y_grid = X_train_scaled, y_train\n",
    "    \n",
    "best_params, best_val_score, all_results = grid_search(\n",
    "        MulticlassLogisticRegression, \n",
    "        param_grid, \n",
    "        X_grid, \n",
    "        y_grid,\n",
    "    )\n",
    "\n",
    "# Training Model Final dengan Hyperparameter Terbaik  dengan Data Training dan Validasi\n",
    "print(\"\\nTraining Final Model\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "final_model = MulticlassLogisticRegression(**best_params)\n",
    "final_model.fit(X_train_scaled, y_train, X_val_scaled, y_val, verbose=1)\n",
    "\n",
    "# Evaluasi Pada Test Data\n",
    "print(\"\\nEvaluasi Final Model dengan Data Test\")\n",
    "test_accuracy = final_model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "y_pred = final_model.predict(X_test_scaled)\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "result = evaluate_model(y_test, y_pred, classes)\n",
    "\n",
    "print(\"Evaluasi model selesai. Hasil tersimpan di 'model_evaluation_results.txt'\")\n",
    "print(\"Plot confusion matrix tersimpan di 'confusion_matrix.png'\")\n",
    "\n",
    "# Visualisasi Bar Chart\n",
    "genres = result['classes']\n",
    "f1_scores = result['f1_score']\n",
    "accuracies = []\n",
    "\n",
    "conf_matrix = result['confusion_matrix']\n",
    "for i in range(len(genres)):\n",
    "    correct = conf_matrix[i][i]\n",
    "    total = np.sum(conf_matrix[i])\n",
    "    acc = correct / total if total > 0 else 0\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "x = np.arange(len(genres))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, accuracies, width, label='Accuracy', color='cornflowerblue')\n",
    "plt.bar(x + width/2, f1_scores, width, label='F1-Score', color='mediumseagreen')\n",
    "\n",
    "plt.xticks(x, genres, rotation=45)\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Akurasi dan F1-Score per Genre')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "\n",
    "for i in range(len(genres)):\n",
    "    plt.text(x[i] - width/2, accuracies[i] + 0.02, f\"{accuracies[i]:.2f}\", ha='center')\n",
    "    plt.text(x[i] + width/2, f1_scores[i] + 0.02, f\"{f1_scores[i]:.2f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('bar_chart.png')\n",
    "plt.close()\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k = 5\n",
    "print(f\"\\nMelakukan {k}-fold Cross Validation untuk analisis stabilitas model\")\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "fold_scores = []\n",
    "\n",
    "# Gabungkan data training dan validation untuk cross-validation\n",
    "X_combined = np.vstack((X_train_scaled, X_val_scaled))\n",
    "y_combined = np.hstack((y_train, y_val))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_combined, y_combined)):\n",
    "    X_fold_train, X_fold_val = X_combined[train_idx], X_combined[val_idx]\n",
    "    y_fold_train, y_fold_val = y_combined[train_idx], y_combined[val_idx]\n",
    "    \n",
    "    model = MulticlassLogisticRegression(**best_params)\n",
    "    model.fit(X_fold_train, y_fold_train, X_fold_val, y_fold_val, verbose=0)\n",
    "    \n",
    "    fold_score = model.evaluate(X_fold_val, y_fold_val, verbose=0)\n",
    "    fold_scores.append(fold_score)\n",
    "    \n",
    "    print(f\"Fold {fold+1}/{k} Accuracy: {fold_score:.4f}\")\n",
    "\n",
    "# Evaluasi setelah seluruh fold\n",
    "mean_score = np.mean(fold_scores)\n",
    "std_score = np.std(fold_scores)\n",
    "print(f\"Akurasi rata-rata: {mean_score:.4f}  {std_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb8ada-7ce3-4550-99ec-d4a7c2a869e2",
   "metadata": {},
   "source": [
    "# Penerapan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17b70e-0df8-4b1f-a758-a7bfba8ffe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import joblib\n",
    "\n",
    "# Load semua komponen\n",
    "loaded_label_encoder = joblib.load('label_encoder.joblib')\n",
    "loaded_tfidf = joblib.load('tfidf_vectorizer.joblib')\n",
    "loaded_pca = joblib.load('pca_model_167.joblib')\n",
    "loaded_model = joblib.load('logistic_regression_model_167.joblib')\n",
    "\n",
    "# Prediksi\n",
    "def predict(text):\n",
    "    X = loaded_tfidf.transform([text])\n",
    "    X_pca = loaded_pca.transform(X.toarray())\n",
    "    pred = loaded_model.predict(X_pca)\n",
    "    return loaded_label_encoder.inverse_transform(pred)[0]\n",
    "\n",
    "# Konfigurasi Halaman\n",
    "st.set_page_config(page_title=\"Klasifikasi Genre Film\", layout=\"centered\")\n",
    "st.title(\" Klasifikasi Genre Film Otomatis\")\n",
    "st.write(\"Gunakan teks sinopsis film atau upload file CSV untuk memprediksi genre menggunakan model Logistic Regression.\")\n",
    "\n",
    "# Tombol Start Klasifikasi\n",
    "if 'start' not in st.session_state:\n",
    "    st.session_state.start = False\n",
    "    \n",
    "if st.button(\" Start Klasifikasi\"):\n",
    "    st.session_state.start = True\n",
    "\n",
    "# Jika Tombol Sudah Ditekan\n",
    "if st.session_state.get(\"start\", False):\n",
    "    st.subheader(\" Masukkan Sinopsis Film atau Upload CSV\")\n",
    "    \n",
    "    tab1, tab2 = st.tabs([\" Teks Manual\", \" Upload CSV\"])\n",
    "    \n",
    "    # Input Teks Manual\n",
    "    with tab1:\n",
    "        input_text = st.text_area(\"Masukkan sinopsis film:\", height=500)\n",
    "        if st.button(\" Klasifikasikan Teks\"):\n",
    "            if input_text.strip() == \"\":\n",
    "                st.warning(\"Teks tidak boleh kosong.\")\n",
    "            else:\n",
    "                result = predict(input_text)\n",
    "                st.success(f\" Prediksi Genre: **{result}**\")\n",
    "    \n",
    "    # Upload CSV\n",
    "    with tab2:\n",
    "        uploaded_file = st.file_uploader(\"Upload file CSV (harus ada kolom 'sinopsis')\", type=[\"csv\"])\n",
    "        if uploaded_file is not None:\n",
    "            try:\n",
    "                df = pd.read_csv(uploaded_file)\n",
    "                if \"sinopsis\" not in df.columns:\n",
    "                    st.error(\"Kolom 'sinopsis' tidak ditemukan dalam file.\")\n",
    "                else:\n",
    "                    # Lakukan prediksi\n",
    "                    texts = df[\"sinopsis\"].fillna(\"\").tolist()\n",
    "                    tfidf_features = loaded_tfidf.transform(texts)\n",
    "                    tfidf_pca = loaded_pca.transform(tfidf_features.toarray())\n",
    "                    predictions = loaded_model.predict(tfidf_pca)\n",
    "                    genres = loaded_label_encoder.inverse_transform(predictions)\n",
    "                    \n",
    "                    df[\"prediksi_genre\"] = genres\n",
    "                    st.subheader(\" Hasil Klasifikasi:\")\n",
    "                    st.dataframe(df[[\"sinopsis\", \"prediksi_genre\"]])\n",
    "                    \n",
    "                    # Visualisasi distribusi genre\n",
    "                    st.subheader(\" Distribusi Genre:\")\n",
    "                    genre_counts = df[\"prediksi_genre\"].value_counts()\n",
    "                    st.bar_chart(genre_counts)\n",
    "                    \n",
    "                    # Tombol download\n",
    "                    csv = df.to_csv(index=False)\n",
    "                    st.download_button(\" Unduh Hasil Klasifikasi\", data=csv, file_name=\"hasil_klasifikasi.csv\", mime=\"text/csv\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Terjadi kesalahan: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
